# Day122

### üîç Text Processing & Tokenization in NLP (Natural Language Processing)

Text processing and tokenization are the **first steps** in any NLP pipeline. These steps prepare raw text for tasks like classification, translation, or chatbot development.

---

## üßº 1. Text Preprocessing Steps

Before tokenization, you usually clean the text. Typical steps include:

| Step                         | Description                        | Example                                 |
| ---------------------------- | ---------------------------------- | --------------------------------------- |
| **Lowercasing**              | Converts all text to lowercase     | `"Hello"` ‚Üí `"hello"`                   |
| **Removing Punctuation**     | Removes symbols like `.`, `,`, `!` | `"hello!"` ‚Üí `"hello"`                  |
| **Removing Stop Words**      | Removes common but useless words   | `"is", "the", "and"`                    |
| **Stemming / Lemmatization** | Reduces words to root form         | `"running"` ‚Üí `"run"`                   |
| **Removing Numbers**         | Often optional                     | `"I have 2 apples"` ‚Üí `"I have apples"` |
| **Whitespace Normalization** | Fixes extra spaces/tabs            | `"  hello   world "` ‚Üí `"hello world"`  |

---

## üî™ 2. Tokenization

**Tokenization** is the process of breaking down text into smaller units ‚Äî **tokens**. These can be:

* **Words** ‚Üí `"I love NLP"` ‚Üí `["I", "love", "NLP"]`
* **Sentences** ‚Üí `"Hi. How are you?"` ‚Üí `["Hi.", "How are you?"]`
* **Subwords** (used in modern NLP models like BERT)

---

## üõ†Ô∏è 3. Code Examples (in Python)

### üîπ Using NLTK

```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
nltk.download('punkt')

text = "Natural Language Processing is awesome! Let's learn it."

# Word Tokenization
words = word_tokenize(text)
print("Words:", words)

# Sentence Tokenization
sentences = sent_tokenize(text)
print("Sentences:", sentences)
```

### üîπ Using spaCy

```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Natural Language Processing is fun and useful.")

# Tokens
tokens = [token.text for token in doc]
print("Tokens:", tokens)
```

---

## üß† 4. Advanced Tokenizers

| Library                    | Features                                        |
| -------------------------- | ----------------------------------------------- |
| **NLTK**                   | Classical NLP, customizable                     |
| **spaCy**                  | Fast, modern, supports named entity recognition |
| **HuggingFace Tokenizers** | Subword tokenization for BERT, GPT, etc.        |
| **gensim**                 | Good for word2vec/tokenization with large texts |

---

## üìå Notes

* Use **subword tokenization** (e.g., BPE) for deep learning models.
* Preprocessing should be **task-dependent**. Don't remove stopwords if you‚Äôre doing sentiment analysis (they might carry meaning).
* Always maintain a **consistent pipeline** during training and inference.

---

