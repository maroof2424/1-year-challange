# *Day 115*
**1. Text Processing**

Text Processing means preparing raw text so it can be used in ML models. Common steps include:

### âœ… Basic Steps:

| Step                 | Description                                           | Example                               |
| -------------------- | ----------------------------------------------------- | ------------------------------------- |
| Lowercasing          | Convert text to lowercase                             | "Hello" â†’ "hello"                     |
| Removing punctuation | Eliminate commas, periods, etc.                       | "Hi, there!" â†’ "Hi there"             |
| Removing stopwords   | Remove common words (e.g., "the", "is", "and")        | "This is a cat" â†’ "cat"               |
| Stemming             | Reducing words to root form (aggressive)              | "playing", "played" â†’ "play"          |
| Lemmatization        | Convert word to its base form (smarter than stemming) | "better" â†’ "good", "was" â†’ "be"       |
| Removing digits      | Remove numbers                                        | "There are 2 dogs" â†’ "There are dogs" |
| Removing whitespaces | Strip extra spaces                                    | " Hello  world " â†’ "Hello world"      |

ðŸ“Œ Python libraries: `re`, `nltk`, `spacy`, `string`

---

## ðŸ”¹ **2. Tokenization**

Tokenization is splitting text into smaller units: **words**, **subwords**, or **sentences**.

### âœ… Types:

| Type     | Description                     | Example                                                           |
| -------- | ------------------------------- | ----------------------------------------------------------------- |
| Word     | Splitting into individual words | "I love Python" â†’ \["I", "love", "Python"]                        |
| Sentence | Splitting into sentences        | "I love NLP. It's powerful." â†’ \["I love NLP.", "It's powerful."] |
| Subword  | For languages or unknown words  | "unhappiness" â†’ \["un", "happi", "ness"]                          |

ðŸ“Œ Python tools: `nltk.word_tokenize`, `nltk.sent_tokenize`, `spacy`, `transformers` (WordPiece, BPE)

---

## âœ… Example in Python (using NLTK):

```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import string

# Download required resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

text = "The quick brown fox jumps over the lazy dog. It's amazing!"

# Lowercase
text = text.lower()

# Remove punctuation
text = text.translate(str.maketrans('', '', string.punctuation))

# Tokenize
words = word_tokenize(text)

# Remove stopwords
words = [w for w in words if w not in stopwords.words('english')]

# Stemming
stemmer = PorterStemmer()
stemmed = [stemmer.stem(w) for w in words]

# Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(w) for w in words]

print("Tokens:", words)
print("Stemmed:", stemmed)
print("Lemmatized:", lemmatized)
```

---

## ðŸ“˜ Use Case in ML

These steps help in:

* Reducing dimensionality
* Normalizing text input
* Improving model performance (especially in NLP tasks like sentiment analysis, classification, chatbots, etc.)

---
